{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef0b2e5",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "After the fine-tuning arc I decided to discover how RAGs are woarking and how to implement them .\n",
    "\n",
    "**What is a RAG ?**\n",
    "\n",
    "A RAG stands for Retrievel augmented generative , it allows to add context and knowledge to an LLM , because of the fact that the knowledge base of an LLM is constrained by what was included in his training data. RAG allows us to intregrate external data as for example PDFs , pictures or others type of document. In addition to that this was for me the best concept to get initiated to Langchain .\n",
    "\n",
    "# How does a RAG work ?\n",
    "\n",
    "There are 5 main steps that we will need to understand to create a RAG .\n",
    "- 1) Loading : we need to load our documents (pdfs,images,csv ...)\n",
    "- 2) Spliting and chunking : we need to split these documents into smaller chunks \n",
    "- 3) Embedding : we need to embbed every chunk that we generated\n",
    "- 4) Storing : we store these embeddings in a vector database \n",
    "- 5) Retrieving : we use the vector store as a retriever , where the LLM will be able to search to find answers to the user's query.\n",
    "\n",
    "If you did not understand every word don't worry I will explain everything .\n",
    "\n",
    "But before to dive deeper into details , I personnally like to have a global idea of how a concept is working , so for that here is a quick explanation of how a RAG works:  \n",
    "- Let's say the input docs are pdfs , we load the pdfs split them into smaller units : chunks\n",
    "- Then we take every chunk and we convert it into a vector of floats : this is what we call embedding\n",
    "- We store these chunks in a Vectore Database (a vectore database is very quick)\n",
    "\n",
    "After these steps , when a user write an input , the input text in converted into a vector of floats (**the embeddings**) and then we use the VectorDB to find the closest vectors (closest in the sens of meanning) . Once we found these vectors we reconvert them into text and send them to the llm with the initial request of the user : we added context to the llm who will answers with this new knowledge.\n",
    "\n",
    "# 1) Loading  and chunking documents\n",
    "\n",
    "For the beginning I will only load a pdf to see how it is working . For this we will need the module text_loader from the langchain_community framework .\n",
    "I decided to use PyMuPdf framework because It extracts the text, completes with headers, lists, and can converts it to Markdown , what is LLM friendly and will allows us to have better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "13ccabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import  DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from typing import List\n",
    "import torch\n",
    "import threading\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough , RunnableLambda\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46989ddf",
   "metadata": {},
   "source": [
    "The thing here is that we want to extract all the pdf documents in a folder , it means that even if the folder contains others directories where there is pdf document we would be able to find them and use them. I wanted to do this to give the user more flexibility and allow him to organize his folder as he wants. \n",
    "\n",
    "For this we will use the DirectoryLoader class that allows us to specity : \n",
    "- **Path** \n",
    "- **Global pattern matching (glob)**\n",
    "- **PdfLoader** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af9452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "separators=[\"\\n\\n\", \"\\n\", \" \", \"\",\"\\n\\n\"]\n",
    "def load_and_chunk(path:str):\n",
    " \"\"\"\n",
    " **path**  \n",
    " \n",
    " path where all the pdf are stocked  \n",
    " **returns** : List[Document] , int\n",
    " \"\"\"\n",
    " assert os.listdir(path) , \"The path does not contain any pdf\"\n",
    " loader = DirectoryLoader(\n",
    "    path= path,\n",
    "    glob = \"**/*.pdf\",\n",
    "    show_progress=True,\n",
    "    recursive= True,\n",
    " )\n",
    "\n",
    " docs = loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter(chunk_size = 100 , chunk_overlap = 10 , separators=separators))\n",
    " print(f\" Successfully generated {len(docs)} chunks\")\n",
    " return docs \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96592946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      " Successfully generated 86 chunks\n",
      "{'source': 'pdf_documents/reglement.pdf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"./pdf_documents\"\n",
    "chunks , nb_chunks= load_and_chunk(path=path)\n",
    "print(chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9310d24",
   "metadata": {},
   "source": [
    "Okay now we have a list of documents where every document contains two elements:  \n",
    "- Metadata (can be useful)\n",
    "- Page_content : the content of the chunked part  \n",
    "\n",
    "If you have followed my until here you may understand that we still have 3 steps to finalize our RAG. Let's see the third step\n",
    "\n",
    "# 2) Step 3 : embbedings \n",
    "\n",
    "Now that we have our chunks , we want to convert them in vectors of float : these vectors are called **embeddings** . This transformation is done through an **embedding model** that aims to capture the meanning of the text/chunk . \n",
    "\n",
    "**But why do wee need this ?**  \n",
    "\n",
    "We need this because we want to store the vectors in a Vector Database (VectoreDB) where we will be able to store and retrieve high-dimensional vector data . And what matters is the fact that in this DB vectors with a **close meanning** are located closer together in the vector space and when our RAG app will receives a user input, it will be embedded and used to query the database, returning the most similar documents.\n",
    "\n",
    "For this I will use the embedding model : \n",
    "`intfloat/multilingual-e5-small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3de54bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This util function will allow us to only load one time the model in our memory to avoid loading it every time \n",
    "# I used a lock to avoid race condition do this function\n",
    "_model_cache = None\n",
    "_model_lock = threading.Lock()\n",
    "def get_model():\n",
    "    global _model_cache\n",
    "    if _model_cache is None:\n",
    "        with _model_lock:  # lock for one thread \n",
    "            if _model_cache is None:  \n",
    "                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "                _model_cache = SentenceTransformer('intfloat/multilingual-e5-small', device=device)\n",
    "    return _model_cache\n",
    "\n",
    "def get_embeddings(docs : List[Document]) :\n",
    "    \"\"\" \n",
    "    **docs**  \n",
    "    The list of chunks created by loading the pdfs  \n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Using cuda \" if device == \"cuda\" else \"Using the cpu\")\n",
    "    model = get_model()\n",
    "    embeddings = model.encode([doc.page_content for doc in docs], convert_to_tensor=True, device=device)\n",
    "    assert len(embeddings) >0 , \"The document is empty: NO EMBEDDING GENERATED\"\n",
    "    print(f\"Successfully generated {len(embeddings)} embeddings\")\n",
    "    #Convert embeddings to a list \n",
    "    embeddings_list = embeddings.tolist()\n",
    "    return embeddings_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d4339",
   "metadata": {},
   "source": [
    "Let's test this sample of code !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f7f64a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda \n",
      "Successfully generated 86 embeddings\n"
     ]
    }
   ],
   "source": [
    "embedding_list = get_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b5a7b",
   "metadata": {},
   "source": [
    "# 3) Step 3 : storing embeddings in a VectoreDB  \n",
    "\n",
    "For this part as I used `ChromaDB` framework for the VectorDB . ChromaDB is an open-source AI application db that gives us everything we need for retrieval :  \n",
    "- Store embeddings and their metadata\n",
    "- Vector search\n",
    "- Full-text search\n",
    "- Document storage\n",
    "- Metadata filtering\n",
    "- Multi-modal retrieval\n",
    "\n",
    "At the beginning I wrote the code mainly with the ChromaDB library , It allows to quickly understand how Chroma works and its main commands. But in reality langchain has a package where he directly integrated chroma , it is : `langchain_chroma`.  \n",
    "\n",
    "So you will find two version of this step , the one with chroma and the other one with langchain, read the one that you want (or both)\n",
    " !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4369d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings(chromaDbPath: str, name_collection: str, chunks : List[Document]):\n",
    "    \"\"\"\n",
    "    **chromaDbPath** \n",
    "    \n",
    "    The path of the persistent client , if it does not exists it creates a new one  \n",
    "    \n",
    "    **name_collection**  \n",
    "    \n",
    "    Name wanted to the collection that we want to create to stock our vectors  \n",
    "    \n",
    "    **chunks**  \n",
    "    \n",
    "    List of chunks generated by loading and chunking the documents  \n",
    "    \n",
    "    \"\"\"\n",
    "    client = chromadb.PersistentClient(path=chromaDbPath)\n",
    "    assert client is not None , \"Problem in getting/creating the client\"\n",
    "    \n",
    "    #Create a collection \n",
    "    collection = client.get_or_create_collection(\n",
    "        name= name_collection,\n",
    "        embedding_function= None #We will use our embeddings\n",
    "    )\n",
    "    \n",
    "    # Create Ids and get embeddings of the document\n",
    "    ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "    embeddings = get_embeddings(chunks)\n",
    "    metadata = [chunk.metadata for chunk in chunks]\n",
    "    documents = [chunk.page_content for chunk in chunks]\n",
    "    try:\n",
    "        collection.add(\n",
    "            ids = ids  ,\n",
    "            embeddings = embeddings,\n",
    "            metadatas=metadata,\n",
    "            documents= documents\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\" Error while adding to ChromaDB : \",e)\n",
    "    print(\"Successfully added all the docs to the vector database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff397b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda \n",
      "Successfully generated 86 embeddings\n",
      "Successfully added all the docs to the vector database\n"
     ]
    }
   ],
   "source": [
    "chromDbPath = './chromaDB'\n",
    "name_collection = \"saad1\"\n",
    "store_embeddings(\n",
    "    chromaDbPath= chromDbPath,\n",
    "    name_collection= name_collection,\n",
    "    chunks=chunks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e19847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vectorDb(query: str,chromaDbPath: str, collection_name :str,n_results: int) -> str:\n",
    "    \n",
    "    assert len(query) >0 , \"Query cannot be empty\"\n",
    "    #Load the client and the db\n",
    "    client = chromadb.PersistentClient(path= chromaDbPath)\n",
    "    collection = client.get_collection(\n",
    "        name= collection_name,\n",
    "        embedding_function=None\n",
    "    )\n",
    "    \n",
    "    doc_query = [Document(metadata={},page_content=query)]\n",
    "    model_name = 'intfloat/multilingual-e5-small'   \n",
    "    embedded_query = get_embeddings(doc_query,model_name)\n",
    "    result = collection.query(\n",
    "        query_embeddings=embedded_query,\n",
    "        n_results= n_results\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528be6fc",
   "metadata": {},
   "source": [
    "Now let try with `langchain-chroma` package .  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_embeddings_with_langchain(collection_name: str , persist_dir:str , chunks : List[Document]):\n",
    "    \n",
    "    \"\"\" \n",
    "    **collection_name**  \n",
    "    Name of the collection that already exists or to create  \n",
    "    \n",
    "    **persist_dir**  \n",
    "    Directory where collection and db are stocked  \n",
    "    \n",
    "    **chunks**  \n",
    "    List of chunks (that under the hood are Documents) generated  \n",
    "    \n",
    "    **returns**  \n",
    "    Chroma instance\n",
    "    \n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "        print(\"Using Cuda to generate embeddings\")\n",
    "    else:\n",
    "        model_kwargs= \"cpu\"\n",
    "        print(\"Using CPU to generate embeddings\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-small\", model_kwargs=model_kwargs)\n",
    "    vector_store = Chroma(\n",
    "        collection_name= collection_name,\n",
    "        embedding_function= embeddings,\n",
    "        persist_directory= persist_dir,\n",
    "    )\n",
    "    \n",
    "    ids=  [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "    try:\n",
    "        vector_store.add_documents(documents=chunks,ids=ids)\n",
    "    except Exception as e:\n",
    "        print(\" Error while adding to ChromaDB : \",e)\n",
    "    print(f\"Successfully added {len(chunks)} vectors in the vector database \")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7dd7729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cuda to generate embeddings\n",
      "Successfully added 86 vectors in the vector database \n"
     ]
    }
   ],
   "source": [
    "vector_store = store_embeddings_with_langchain(\n",
    "    collection_name=name_collection,\n",
    "    persist_dir=chromDbPath,\n",
    "    chunks=chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e55daae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='chunk_17', metadata={'source': 'pdf_documents/reglement.pdf'}, page_content='1.2. Organisation'),\n",
       " Document(id='chunk_64', metadata={'source': 'pdf_documents/reglement.pdf'}, page_content='se voir sanctionnés. Les degrés de sanctions applicables sont présentés plus loin.'),\n",
       " Document(id='chunk_79', metadata={'source': 'pdf_documents/reglement.pdf'}, page_content='Les sanctions sont appliquées par le Bureau et varient selon la gravité de la faute. Elles peuvent'),\n",
       " Document(id='chunk_13', metadata={'source': 'pdf_documents/reglement.pdf'}, page_content='de redoublement et de césure.')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search(\"Danger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3646db",
   "metadata": {},
   "source": [
    "Now the function to query our vectore store is less complex because Langchain do everything for us with the `Chroma Class`.\n",
    "We will create a pipeline by using one of the most intersting features of Langchain : `chains` and more precisely , we will create a **LCEL**: which stands for <u>LangChain Expression Language</u>.  \n",
    "\n",
    "And here you may ask yourself he is talking about chains , but what these chains are linking ? The answer is : **runnables**. \n",
    "\n",
    "My next post will be about Runnables in Langchain so here I will just try to give a quick explanation of how it's working and why it used .  \n",
    "\n",
    "# Runnables \n",
    "\n",
    "A **runnable** in Langchain is a the basic block/component that can be linked to other object of type Runnables . In fact **Runnables** is an abstract class in LangChain that is herited by others components (PromptTemplate, ChatOpenAI ....) and it provides methods that allows Runnables objects to connect between them : it's a link in the chain . Runnables object can also be executed with `.invoke()` method.  \n",
    "\n",
    "And this is the power of LangChain , it allows us to seamlessly integrate various components required for building workflows and ensures that all components follow the same set of rules and can easily connect, which simplifies the development process. In addition there Runnables primitives that supports different workflows : parallel , sequential and also conditional executions.  \n",
    "\n",
    "Okay now we can see the functionnement of a **chain**:  \n",
    "$$\n",
    "chain = Runnables1 | Runnables2 | .... | RunnablesN\n",
    "$$\n",
    "The ` | ` operator here takes the output of the previous Runnables and pass it as an entry to the next one. \n",
    "\n",
    "Now let's code our final step by using chains !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "533322ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_answer(query: str , vector_store: Chroma ):\n",
    "    \n",
    "    load_dotenv()\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature = 0.5,\n",
    "        max_retries = 2\n",
    "    )\n",
    "\n",
    "    docs_retriever = RunnableLambda(lambda query: vector_store.similarity_search(query=query,k=6))\n",
    "    docs_to_text = RunnableLambda(lambda docs: \"\\n\\n\".join(doc.page_content for doc in docs))\n",
    "    query_passthrough = RunnablePassthrough()\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \n",
    "        \"\"\"Use the following context to answer the question at the end. \n",
    "           You must be respectful and helpful, and answer in the language of the question.\n",
    "           If you don't know the answer, say that you don't know.\n",
    "\n",
    "           Context: {context}\n",
    "\n",
    "           Question: {question}\"\"\"\n",
    "    )\n",
    "    prompt_runnable = RunnableLambda(lambda args: prompt_template.format_messages(context=args[\"context\"], question=args[\"query\"]))\n",
    "    \n",
    "    pipeline = (\n",
    "        {\n",
    "            \"context\": docs_retriever|docs_to_text,\n",
    "            \"query\": query_passthrough\n",
    "        }\n",
    "        | prompt_runnable\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    answer = pipeline.invoke(query)\n",
    "    return answer\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a8ab2468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Le document parle d'une organisation ou d'une association, vraisemblablement liée à l'ENSEEIHT, qui a pour objectif de développer des liens entre les personnes qui y sont attachées. Il aborde également des sujets tels que le montant de la cotisation, les sanctions applicables et fait référence à des campagnes pour les années 2021-2022.\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'De quoi parle le document ?'\n",
    "vector_store = vector_store\n",
    "\n",
    "answer = get_rag_answer(query,vector_store)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9abd17c",
   "metadata": {},
   "source": [
    "Everything is working now it's time to put together all of the functions that we wrote !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f7c8ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query:str):\n",
    "    collection_name = \"tcp\"\n",
    "    path = \"./pdf_documents\"\n",
    "    persitent_dir = './chromaDB'\n",
    "    \n",
    "    chunks = load_and_chunk(path)\n",
    "    vector_store = store_embeddings_with_langchain(\n",
    "        collection_name,\n",
    "        persitent_dir,\n",
    "        chunks\n",
    "    )\n",
    "    answer = get_rag_answer(query,vector_store)\n",
    "    pprint.pp(answer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e464990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      " Successfully generated 138 chunks\n",
      "Using Cuda to generate embeddings\n",
      " Error while adding to ChromaDB :  'list' object has no attribute 'page_content'\n",
      "Successfully added 2 vectors in the vector database \n",
      "(\"Je ne peux pas vous dire de quoi parle le document car aucun contexte n'a \"\n",
      " 'été fourni.')\n"
     ]
    }
   ],
   "source": [
    "answer = ask_question(\"De quoi parle le document?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
