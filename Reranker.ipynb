{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99dd5a95",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "\n",
    "In my last post, I built a RAG using LangChain. After testing it, I quickly realized the results weren’t great. That pushed me to look for ways to improve its performance. One of the first things that came to mind was a startup called **ZeroEntropy**, which works on something known as **reranking** .\n",
    "So I decided to dig into this concept: **Rerankers**.\n",
    "\n",
    "# The problem \n",
    "\n",
    "Imagine searching for **'football shoes'** in Google . The search engine will return to you hundreds of options , some of them will be highly relevant and others not so much . You will find sneakers or even tennis sandals.  \n",
    "And the solution to face this issue is to use a **Reranker** that will reorder results to put the most relevant items at the top of the page (for example CR7 -the goat- shoes) and at the bottom , the less useful ones (golf shoes for example).\n",
    "\n",
    "We have the same issue with RAGs . In fact in a naive RAG pipeline, the user’s query is embedded and sent to a retriever. The retriever then returns the documents whose embeddings are closest to the query’s embedding according to a similarity measure. But the thing is that the retriever can and do provides irrelevant documents which leads to poor-quality answers .  \n",
    "The solution ? **RERANKER** , after that we for example retrieve 10 relevant documents to the query we reorder them to only use the most relevant . Here is a quick a recap to make it clear:  \n",
    "- **Step1 : Broad Retrieval**  \n",
    "The retriever pulls a large of potential document , it quick but not accurate\n",
    "\n",
    "- **Step2 : Reranking**  \n",
    "The reranker examines each of the retrieved documents alongsie the user's query and assigns precise relevance score to esach quer-document pair and then the documents are reordered based on this score : it's prioritizes accuracy.\n",
    "\n",
    "\n",
    "# What is a Reranker ?  \n",
    "\n",
    "A reranker is neither more nor less an **Encoder** , to be more precise a Cross-encoder . In the broad retrieval step , we embedd the query and the document separtly with a classical encoder : the embedding model , and then we retrieve the k most relevant documents . After this we took every document and we calculate it's embbeding **alongside** the query and then we pass this embedding in a **scoring head** that will returns a **relevance score**. \n",
    "\n",
    "Here is a quick example to show you what happens:\n",
    "\n",
    "**Step1 : create the pair query/document**\n",
    "$$\n",
    "texttoscore = [cls]\\;query\\;[sep]\\;document_{i}\\;[sep]\n",
    "$$\n",
    "**Step2: generate its embedding**\n",
    "$$\n",
    "embedding_{i} = embedding(texttoscore)\n",
    "$$\n",
    "**Step3 : compute the relevance score**\n",
    "\n",
    "$$\n",
    "crossEncoderScore =  relevanceScore(embedding_{i})\n",
    "$$\n",
    "\n",
    "**Step 4/5: Reorder**  \n",
    "\n",
    "The reranker reorder the documents based on relevance scores .\n",
    "\n",
    "**Okay now that we know how rerankers work let code this and see if the answers are better !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3f8c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
