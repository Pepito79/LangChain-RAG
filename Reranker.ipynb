{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99dd5a95",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "\n",
    "In my last post, I built a RAG using LangChain. After testing it, I quickly realized the results weren’t great. That pushed me to look for ways to improve its performance. One of the first things that came to mind was a startup called **ZeroEntropy**, which works on something known as **reranking** .\n",
    "So I decided to dig into this concept: **Rerankers**.\n",
    "\n",
    "# The problem \n",
    "\n",
    "Imagine searching for **'football shoes'** in Google . The search engine will return to you hundreds of options , some of them will be highly relevant and others not so much . You will find sneakers or even tennis sandals.  \n",
    "And the solution to face this issue is to use a **Reranker** that will reorder results to put the most relevant items at the top of the page (for example CR7 -the goat- shoes) and at the bottom , the less useful ones (golf shoes for example).\n",
    "\n",
    "We have the same issue with RAGs . In fact in a naive RAG pipeline, the user’s query is embedded and sent to a retriever. The retriever then returns the documents whose embeddings are closest to the query’s embedding according to a similarity measure. But the thing is that the retriever can and do provides irrelevant documents which leads to poor-quality answers .  \n",
    "The solution ? **RERANKER** , after that we for example retrieve 10 relevant documents to the query we reorder them to only use the most relevant . Here is a quick a recap to make it clear:  \n",
    "- **Step1 : Broad Retrieval**  \n",
    "The retriever pulls a large of potential document , it quick but not accurate\n",
    "\n",
    "- **Step2 : Reranking**  \n",
    "The reranker examines each of the retrieved documents alongsie the user's query and assigns precise relevance score to esach quer-document pair and then the documents are reordered based on this score : it's prioritizes accuracy.\n",
    "\n",
    "\n",
    "# What is a Reranker ?  \n",
    "\n",
    "A reranker is neither more nor less an **Encoder** , to be more precise a Cross-encoder . In the broad retrieval step , we embedd the query and the document separtly with a classical encoder : the embedding model , and then we retrieve the k most relevant documents . After this we took every document and we calculate it's embbeding **alongside** the query and then we pass this embedding in a **scoring head** that will returns a **relevance score**. \n",
    "\n",
    "Here is a quick example to show you what happens:\n",
    "\n",
    "**Step1 : create the pair query/document**\n",
    "$$\n",
    "texttoscore = [cls]\\;query\\;[sep]\\;document_{i}\\;[sep]\n",
    "$$\n",
    "**Step2: generate its embedding**\n",
    "$$\n",
    "embedding_{i} = embedding(texttoscore)\n",
    "$$\n",
    "**Step3 : compute the relevance score**\n",
    "\n",
    "$$\n",
    "crossEncoderScore =  relevanceScore(embedding_{i})\n",
    "$$\n",
    "\n",
    "**Step 4/5: Reorder**  \n",
    "\n",
    "The reranker reorder the documents based on relevance scores .\n",
    "\n",
    "**Okay now that we know how rerankers work let code this and see if the answers are better !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ec1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_chroma import Chroma\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import hashlib\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda,RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rag:\n",
    "    def __init__(self, name:str):\n",
    "        self.name = name\n",
    "    \n",
    "    def load_documents(self,path:str) -> List[Document]:\n",
    "        self.path = path \n",
    "        loader = DirectoryLoader(\n",
    "            path=path,\n",
    "            glob=\"**/*.pdf\",\n",
    "            show_progress=True,\n",
    "            recursive=True\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        return documents\n",
    "    \n",
    "    def split_documents(self):\n",
    "        doc_list = self.load_documents(self.path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "        textSplitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "            tokenizer=tokenizer,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\",\"\\n\\n\"],\n",
    "            chunk_size = 512,\n",
    "            chunk_overlap = int(512/10),\n",
    "            add_start_index = True,               \n",
    "            strip_whitespace = True ,\n",
    "        )\n",
    "        chunks = textSplitter.split_documents(doc_list)\n",
    "        self.len_chunks = len(chunks)\n",
    "        return chunks\n",
    "     \n",
    "    def createVectorStore(self ,collection_name:str, persist_dir:str, chunks: List[Document]):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_dir = persist_dir\n",
    "        if torch.cuda.is_available():\n",
    "            model_kwargs = {\"device\": \"cuda\"}\n",
    "            print(\"Using Cuda to generate embeddings\")\n",
    "        else:\n",
    "            model_kwargs=  {\"device\": \"cpu\"}\n",
    "            print(\"Using CPU to generate embeddings\")\n",
    "            \n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-small\", model_kwargs=model_kwargs)\n",
    "        \n",
    "        vector_store = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embeddings,\n",
    "            persist_directory=self.persist_dir\n",
    "        )\n",
    "        \n",
    "        current_content = vector_store.get()\n",
    "        existing_ids = set(current_content[\"ids\"]) if current_content[\"ids\"] else set()\n",
    "        \n",
    "        docs_to_add = []\n",
    "        ids_to_add = []\n",
    "        \n",
    "        for doc in chunks:\n",
    "            doc_id = hashlib.md5(doc.page_content.encode()).hexdigest()\n",
    "            \n",
    "            #Verify if the id is alread in the db\n",
    "            if doc_id not in existing_ids:\n",
    "                docs_to_add.append(doc)\n",
    "                ids_to_add.append(doc_id)\n",
    "        if len(docs_to_add) == 0:\n",
    "            print(\"Documents already in the DB nothing will be added to the vector store\")\n",
    "            return vector_store\n",
    "        try:\n",
    "            vector_store.add_documents(documents=docs_to_add,ids=ids_to_add)\n",
    "        except Exception as e:\n",
    "            print(\" Error while adding to ChromaDB : \",e)\n",
    "        print(f\"Successfully added {len(docs_to_add)} vectors in the vector database \")\n",
    "        return vector_store\n",
    "    \n",
    "    \n",
    "    def late_interaction(self, query:str, docs :List[Document]):\n",
    "        pass \n",
    "    \n",
    "    def answer_query(self,query:str, vector_store: Chroma, k_retrieval: int,k_reranker:int,reranker: bool = False ):\n",
    "        \n",
    "        if k_reranker > self.len_chunks:\n",
    "            raise ValueError(f\"k_reranker {k_reranker} can not be greater than the number of chuns {self.len_chunks}\")\n",
    "        \n",
    "        if k_retrieval > self.len_chunks:\n",
    "            raise ValueError(f\"k {k_retrieval} can not be greater than the number of chuns {self.len_chunks}\")\n",
    "        \n",
    "        retrieved_doc = vector_store.similarity_search(query,k_retrieval)\n",
    "        \n",
    "        load_dotenv()\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            temperature = 0.5,\n",
    "            max_retries = 2\n",
    "        )\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Use the following context to answer the question at the end. \n",
    "           You must be respectful and helpful, and answer in the language of the question.\n",
    "           If you don't know the answer, say that you don't know.\n",
    "\n",
    "           Context: {context}\n",
    "\n",
    "           Question: {question}\n",
    "           \"\"\"\n",
    "        )   \n",
    "        runnable_query = RunnablePassthrough()                         \n",
    "        prompt_runnable = RunnableLambda(lambda args: prompt.format_messages(context = args[\"context\"] , question=args[\"question\"]))\n",
    "        if reranker:\n",
    "            model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "            chunks_content = [chunk.page_content for chunk in retrieved_doc]\n",
    "            ranks = model.rank(query,chunks_content,k_reranker)\n",
    "            context = RunnableLambda(lambda _ :\"\\n\\n\".join(chunks_content[rank[\"corpus_id\"]] for rank in (ranks)))\n",
    "            pipeline = (\n",
    "            {\n",
    "                \"context\":context,\n",
    "                \"question\" : runnable_query\n",
    "            }\n",
    "            | prompt_runnable\n",
    "            |llm\n",
    "            |StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        else:\n",
    "            context = RunnableLambda(lambda doc :\"\\n\\n\".join(doc.page_content for doc in retrieved_doc ))\n",
    "\n",
    "            pipeline = (\n",
    "                {\n",
    "                    \"context\":context,\n",
    "                    \"question\" : runnable_query\n",
    "                }\n",
    "                | prompt_runnable\n",
    "                |llm\n",
    "                |StrOutputParser()\n",
    "            )\n",
    "            \n",
    "        answer = pipeline.invoke(query)\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e88e0367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cuda to generate embeddings\n",
      "Documents already in the DB nothing will be added to the vector store\n"
     ]
    }
   ],
   "source": [
    "rag_1 = Rag(\"Saad_rag\")\n",
    "docs = rag_1.load_documents(path=\"/home/pepito/Documents/Python/ML/GenAI/RAG/pdf_documents\")\n",
    "chunks = rag_1.split_documents()\n",
    "vector_store = rag_1.createVectorStore(\n",
    "    collection_name=\"test_1\",\n",
    "    persist_dir=\"/home/pepito/Documents/Python/ML/GenAI/RAG/persist_dir_oop\",\n",
    "    chunks=chunks\n",
    ")\n",
    "\n",
    "answer = rag_1.answer_query(\n",
    "    query=\"De quoi parle le document\",\n",
    "    vector_store=vector_store,\n",
    "    k_retrieval=6,\n",
    "    k_reranker=3,\n",
    "    reranker=True\n",
    ")\n",
    "\n",
    "\n",
    "answer_no_reranking = rag_1.answer_query(\n",
    "    query=\"De quoi parle le document\",\n",
    "    vector_store=vector_store,\n",
    "    k_retrieval=6,\n",
    "    k_reranker = 0,\n",
    "    reranker=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75704edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Le document parle des adresses IP (IPv4 et IPv6), de leur structure, du '\n",
      " 'masque de sous-réseau, du protocole TCP/IP et du routage des données sur '\n",
      " 'Internet. Il explique comment les données sont acheminées sous forme de '\n",
      " 'paquets entre les machines via des routeurs.')\n",
      "************************************************************************************************************************\n",
      "('Le document parle principalement des protocoles **TCP (Transmission Control '\n",
      " \"Protocol)** et **IP (Internet Protocol)**, réunis sous l'appellation \"\n",
      " \"**TCP/IP**, qui sont fondamentaux pour le fonctionnement d'Internet et \"\n",
      " \"l'échange de données entre ordinateurs.\\n\"\n",
      " '\\n'\n",
      " 'Il détaille le rôle de chaque protocole :\\n'\n",
      " '*   Le **TCP** assure la fiabilité de la transmission des paquets de données '\n",
      " \"(segments), leur numérotation, l'envoi d'accusés de réception et la \"\n",
      " \"reconstitution des fichiers à l'arrivée.\\n\"\n",
      " \"*   L'**IP** gère l'adressage unique des machines sur le réseau (avec les \"\n",
      " \"versions IPv4 et IPv6), l'identification des réseaux et le routage des \"\n",
      " 'paquets de données vers leur destinataire.\\n'\n",
      " '\\n'\n",
      " 'Le document explique également le concept de **routage**, décrivant comment '\n",
      " 'les données sont transférées sous forme de paquets entre machines via des '\n",
      " '**routeurs** et des **switchs**, et comment ces équipements utilisent les '\n",
      " 'adresses IP pour acheminer les informations à travers différents réseaux. Il '\n",
      " \"aborde la structure des adresses IP (ID de réseau et ID d'hôte), les masques \"\n",
      " 'de sous-réseau, et le fonctionnement des réseaux locaux.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pp(answer)\n",
    "print(\"*\"*120)\n",
    "pprint.pp(answer_no_reranking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
